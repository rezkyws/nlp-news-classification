# -*- coding: utf-8 -*-
"""nlp_submission_v3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qj6P1z_SjAN_lGeKPSS3hB4R0L9O9kD0
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
os.environ['KAGGLE_CONFIG_DIR'] = "/content/gdrive/My Drive/Kaggle"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/My Drive/Kaggle

!kaggle datasets download -d amananandrai/ag-news-classification-dataset

!ls

# !unzip \*.zip  && rm *.zip

!unzip \*ag-news-classification-dataset.zip && rm ag-news-classification-dataset.zip

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
# %matplotlib inline
import matplotlib.pyplot as plt
import re
import nltk
from nltk.corpus import stopwords
from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,TensorBoard,CSVLogger,ReduceLROnPlateau,LearningRateScheduler
from tensorflow.keras.optimizers import RMSprop

df = pd.read_csv('train.csv')
df

# Decided to use title and description coloumn as independent variable and merge that coloumns

df['text'] = df['Title'] + " " + df['Description']
df['class_index'] = df['Class Index']
df = df.drop(columns=['Title', 'Description', 'Class Index'])
df

# Cleaning the data from stopwords with some whitelist

nltk.download('stopwords')
def remove_stopwords(input_text):
        stopwords_list = stopwords.words('english')
        whitelist = ["n't", "not", "no"]
        words = input_text.split() 
        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] 
        return " ".join(clean_words) 
       
df.text = df.text.apply(remove_stopwords)
df

# Vizualisation/plot loss and accuracy function

def show_final_history(history):
    fig, ax = plt.subplots(1, 2, figsize=(15,5))
    ax[0].set_title('loss')
    ax[0].plot(history.epoch, history.history["loss"], label="Train loss")
    ax[0].plot(history.epoch, history.history["val_loss"], label="Validation loss")
    ax[1].set_title('accuracy')
    ax[1].plot(history.epoch, history.history["accuracy"], label="Train acc")
    ax[1].plot(history.epoch, history.history["val_accuracy"], label="Validation acc")
    ax[0].legend()
    ax[1].legend()

# One-hot encoding

category = pd.get_dummies(df.class_index)
df_new = pd.concat([df, category], axis=1)
df_new = df_new.drop(columns='class_index')
df_new
# label : 1-World, 2-Sports, 3-Business, 4-Sci/Tech

# Separate attributes and labels

text = df_new['text'].values
label = df_new[[1, 2, 3, 4]]

from sklearn.model_selection import train_test_split
text_train, text_test, label_train, label_test = train_test_split(text, label, test_size=0.2)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Tokenization
     
tokenizer = Tokenizer(num_words=10000, oov_token='x', filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n', lower=True)
tokenizer.fit_on_texts(text_train) 
tokenizer.fit_on_texts(text_test)

# Sequencing and padding

sequences_train = tokenizer.texts_to_sequences(text_train)
sequences_test = tokenizer.texts_to_sequences(text_test)
padded_train = pad_sequences(sequences_train) 
padded_test = pad_sequences(sequences_test)

import tensorflow as tf

model = tf.keras.Sequential([
        tf.keras.layers.Embedding(input_dim=10000, output_dim=16),
        tf.keras.layers.LSTM(128, return_sequences=True),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.LSTM(64),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(4, activation='softmax')
])

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

# (Callbacks) check the accuration of the model if already over 90% then stop training

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9):
      print("\nAccuration's reached > 90%!")
      self.model.stop_training = True
callbacks = myCallback()

num_epochs = 20
history = model.fit(padded_train, label_train, epochs=num_epochs, batch_size=512,
                    validation_data=(padded_test, label_test), verbose=1, callbacks=[callbacks])

show_final_history(history)
model_json = model.to_json()